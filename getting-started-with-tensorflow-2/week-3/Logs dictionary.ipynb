{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"Logs dictionary.ipynb","provenance":[{"file_id":"1jepIhnWcguzdltyGkufBsf_Toz6wbhK_","timestamp":1600648273798}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"p5_0GOqVEkuZ","colab_type":"text"},"source":["# Using the logs dictionary\n","\n","In this reading, we will learn how to take advantage of the `logs` dictionary in Keras to define our own callbacks and check the progress of a model."]},{"cell_type":"code","metadata":{"id":"hmK4bae2Eq2O","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1600648325307,"user_tz":240,"elapsed":28210,"user":{"displayName":"Halil Uysal","photoUrl":"","userId":"03813492140216734474"}},"outputId":"87e61909-6db2-4cdc-a67a-b47dda978bc0"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ii60T6ChEkuZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1600818756544,"user_tz":240,"elapsed":2277,"user":{"displayName":"Halil Uysal","photoUrl":"","userId":"03813492140216734474"}},"outputId":"55c5b0ed-6596-4fab-e8d4-b77e0210d1d1"},"source":["import tensorflow as tf\n","print(tf.__version__)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["2.3.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YE6zhxxqEkuc","colab_type":"text"},"source":["The `logs` dictionary stores the loss value, along with all of the metrics we are using at the end of a batch or epoch.\n","\n","We can incorporate information from the `logs` dictionary into our own custom callbacks.\n","\n","Let's see this in action in the context of a model we will construct and fit to the `sklearn` diabetes dataset that we have been using in this module.\n","\n","Let's first import the dataset, and split it into the training and test sets."]},{"cell_type":"code","metadata":{"id":"jSDN6S3uEkuc","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600819182277,"user_tz":240,"elapsed":1060,"user":{"displayName":"Halil Uysal","photoUrl":"","userId":"03813492140216734474"}}},"source":["# Load the diabetes dataset\n","\n","from sklearn.datasets import load_diabetes\n","\n","diabetes_dataset = load_diabetes()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"5xyPLGQ-Ekue","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600819191899,"user_tz":240,"elapsed":467,"user":{"displayName":"Halil Uysal","photoUrl":"","userId":"03813492140216734474"}}},"source":["# Save the input and target variables\n","\n","from sklearn.model_selection import train_test_split\n","\n","data = diabetes_dataset['data']\n","targets = diabetes_dataset['target']"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"up4r-2laEkug","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600819195678,"user_tz":240,"elapsed":553,"user":{"displayName":"Halil Uysal","photoUrl":"","userId":"03813492140216734474"}}},"source":["# Split the data set into training and test sets\n","\n","train_data, test_data, train_targets, test_targets = train_test_split(data, targets, test_size=0.1)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WGI3nfqyEkui","colab_type":"text"},"source":["Now we construct our model."]},{"cell_type":"code","metadata":{"id":"zIMMe8bUEkui","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600819205987,"user_tz":240,"elapsed":796,"user":{"displayName":"Halil Uysal","photoUrl":"","userId":"03813492140216734474"}}},"source":["# Build the model\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","\n","model = tf.keras.Sequential([\n","    Dense(128, activation='relu', input_shape=(train_data.shape[1],)),\n","    Dense(64,activation='relu'),\n","    tf.keras.layers.BatchNormalization(),\n","    Dense(64, activation='relu'),\n","    Dense(64, activation='relu'),\n","    Dense(1)        \n","])"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RMr7Q05vEkuk","colab_type":"text"},"source":["We now compile the model, with\n","* Mean squared error as the loss function,\n","* the Adam optimizer, and \n","* Mean absolute error (`mae`) as a metric."]},{"cell_type":"code","metadata":{"id":"iDQLzY_vEkuk","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600819229048,"user_tz":240,"elapsed":451,"user":{"displayName":"Halil Uysal","photoUrl":"","userId":"03813492140216734474"}}},"source":["# Compile the model\n","    \n","model.compile(loss='mse', optimizer=\"adam\", metrics=['mae'])"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-ggf7YxCEkum","colab_type":"text"},"source":["### Defining a custom callback\n","\n","Now we define our custom callback using the `logs` dictionary to access the loss and metric values."]},{"cell_type":"code","metadata":{"id":"15SyDdSoEkun","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600819296621,"user_tz":240,"elapsed":534,"user":{"displayName":"Halil Uysal","photoUrl":"","userId":"03813492140216734474"}}},"source":["# Create the custom callback\n","\n","class LossAndMetricCallback(tf.keras.callbacks.Callback):\n","\n","    # Print the loss after every second batch in the training set\n","    def on_train_batch_end(self, batch, logs=None):\n","        if batch % 2 ==0:\n","            print('\\n After batch {}, the loss is {:7.2f}.'.format(batch, logs['loss']))\n","    \n","    # Print the loss after each batch in the test set\n","    def on_test_batch_end(self, batch, logs=None):\n","        print('\\n After batch {}, the loss is {:7.2f}.'.format(batch, logs['loss']))\n","\n","    # Print the loss and mean absolute error after each epoch\n","    def on_epoch_end(self, epoch, logs=None):\n","        print('Epoch {}: Average loss is {:7.2f}, mean absolute error is {:7.2f}.'.format(epoch, logs['loss'], logs['mae']))\n","    \n","    # Notify the user when prediction has finished on each batch\n","    def on_predict_batch_end(self,batch, logs=None):\n","        print(\"Finished prediction on batch {}!\".format(batch))"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sk6VIZGYEkuo","colab_type":"text"},"source":["We now fit the model to the data, and specify that we would like to use our custom callback `LossAndMetricCallback()`."]},{"cell_type":"code","metadata":{"id":"tdXuhHVXEkup","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1600819312779,"user_tz":240,"elapsed":1794,"user":{"displayName":"Halil Uysal","photoUrl":"","userId":"03813492140216734474"}},"outputId":"323c255b-e8c0-44b1-ede1-f179c617f157"},"source":["# Train the model\n","\n","history = model.fit(train_data, train_targets, epochs=20,\n","                    batch_size=100, callbacks=[LossAndMetricCallback()], verbose=False)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["\n"," After batch 0, the loss is 27244.92.\n","\n"," After batch 2, the loss is 27882.17.\n","Epoch 0: Average loss is 29251.56, mean absolute error is  152.62.\n","\n"," After batch 0, the loss is 30223.42.\n","\n"," After batch 2, the loss is 29905.89.\n","Epoch 1: Average loss is 29133.72, mean absolute error is  152.27.\n","\n"," After batch 0, the loss is 28636.50.\n","\n"," After batch 2, the loss is 29091.59.\n","Epoch 2: Average loss is 28968.71, mean absolute error is  151.77.\n","\n"," After batch 0, the loss is 24653.38.\n","\n"," After batch 2, the loss is 28369.45.\n","Epoch 3: Average loss is 28725.94, mean absolute error is  151.06.\n","\n"," After batch 0, the loss is 27476.09.\n","\n"," After batch 2, the loss is 28383.57.\n","Epoch 4: Average loss is 28390.92, mean absolute error is  150.05.\n","\n"," After batch 0, the loss is 26092.46.\n","\n"," After batch 2, the loss is 27727.44.\n","Epoch 5: Average loss is 27932.28, mean absolute error is  148.71.\n","\n"," After batch 0, the loss is 27164.55.\n","\n"," After batch 2, the loss is 27341.64.\n","Epoch 6: Average loss is 27324.08, mean absolute error is  146.92.\n","\n"," After batch 0, the loss is 27582.95.\n","\n"," After batch 2, the loss is 25290.12.\n","Epoch 7: Average loss is 26537.65, mean absolute error is  144.61.\n","\n"," After batch 0, the loss is 25916.91.\n","\n"," After batch 2, the loss is 25956.88.\n","Epoch 8: Average loss is 25568.19, mean absolute error is  141.71.\n","\n"," After batch 0, the loss is 25290.34.\n","\n"," After batch 2, the loss is 25093.91.\n","Epoch 9: Average loss is 24369.69, mean absolute error is  138.05.\n","\n"," After batch 0, the loss is 21285.31.\n","\n"," After batch 2, the loss is 22564.37.\n","Epoch 10: Average loss is 22935.95, mean absolute error is  133.58.\n","\n"," After batch 0, the loss is 22439.47.\n","\n"," After batch 2, the loss is 22618.73.\n","Epoch 11: Average loss is 21320.58, mean absolute error is  128.00.\n","\n"," After batch 0, the loss is 16825.93.\n","\n"," After batch 2, the loss is 18129.83.\n","Epoch 12: Average loss is 19351.86, mean absolute error is  121.20.\n","\n"," After batch 0, the loss is 18601.10.\n","\n"," After batch 2, the loss is 17798.30.\n","Epoch 13: Average loss is 17207.39, mean absolute error is  113.13.\n","\n"," After batch 0, the loss is 15949.15.\n","\n"," After batch 2, the loss is 15037.77.\n","Epoch 14: Average loss is 15050.73, mean absolute error is  103.94.\n","\n"," After batch 0, the loss is 15006.51.\n","\n"," After batch 2, the loss is 12802.73.\n","Epoch 15: Average loss is 12697.29, mean absolute error is   92.84.\n","\n"," After batch 0, the loss is 11623.53.\n","\n"," After batch 2, the loss is 10531.76.\n","Epoch 16: Average loss is 10524.34, mean absolute error is   82.90.\n","\n"," After batch 0, the loss is 8761.98.\n","\n"," After batch 2, the loss is 9461.93.\n","Epoch 17: Average loss is 8611.83, mean absolute error is   73.48.\n","\n"," After batch 0, the loss is 5474.85.\n","\n"," After batch 2, the loss is 7142.56.\n","Epoch 18: Average loss is 7217.55, mean absolute error is   66.91.\n","\n"," After batch 0, the loss is 7360.43.\n","\n"," After batch 2, the loss is 6351.75.\n","Epoch 19: Average loss is 6441.83, mean absolute error is   63.00.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"X9iP9JizEkuq","colab_type":"text"},"source":["We can also use our callback in the `evaluate` function..."]},{"cell_type":"code","metadata":{"id":"ZyOwPbPFEkur","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":207},"executionInfo":{"status":"ok","timestamp":1600819357802,"user_tz":240,"elapsed":486,"user":{"displayName":"Halil Uysal","photoUrl":"","userId":"03813492140216734474"}},"outputId":"1b8cc4fa-55e8-4c55-8ba1-bd4c5b709f36"},"source":["# Evaluate the model\n","\n","model_eval = model.evaluate(test_data, test_targets, batch_size=10, \n","                            callbacks=[LossAndMetricCallback()], verbose=False)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["\n"," After batch 0, the loss is 14722.79.\n","\n"," After batch 1, the loss is 13026.15.\n","\n"," After batch 2, the loss is 17407.56.\n","\n"," After batch 3, the loss is 17412.52.\n","\n"," After batch 4, the loss is 16329.91.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AXr6jeg2Ekus","colab_type":"text"},"source":["...And also the `predict` function."]},{"cell_type":"code","metadata":{"id":"WgyJGZiwEkut","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":112},"executionInfo":{"status":"ok","timestamp":1600819362768,"user_tz":240,"elapsed":428,"user":{"displayName":"Halil Uysal","photoUrl":"","userId":"03813492140216734474"}},"outputId":"809d88fc-9ffc-4e2b-dea4-aaf01df57e49"},"source":["# Get predictions from the model\n","\n","model_pred = model.predict(test_data, batch_size=10,\n","                           callbacks=[LossAndMetricCallback()], verbose=False)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Finished prediction on batch 0!\n","Finished prediction on batch 1!\n","Finished prediction on batch 2!\n","Finished prediction on batch 3!\n","Finished prediction on batch 4!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qpUoluIvEkuv","colab_type":"text"},"source":["### Application - learning rate scheduler\n","Let's now look at a more sophisticated custom callback. \n","\n","We are going to define a callback to change the learning rate of the optimiser of a model during training. We will do this by specifying the epochs and new learning rates where we would like it to be changed.\n","\n","First we define the auxillary function that returns the learning rate for each epoch based on our schedule."]},{"cell_type":"code","metadata":{"id":"odsnPFhzEkuv","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600819567544,"user_tz":240,"elapsed":476,"user":{"displayName":"Halil Uysal","photoUrl":"","userId":"03813492140216734474"}}},"source":["# Define the learning rate schedule. The tuples below are (start_epoch, new_learning_rate)\n","\n","lr_schedule = [\n","    (4, 0.03), (7, 0.02), (11, 0.005), (15, 0.007)\n","]\n","\n","def get_new_epoch_lr(epoch, lr):\n","    # Checks to see if the input epoch is listed in the learning rate schedule \n","    # and if so, returns index in lr_schedule\n","    epoch_in_sched = [i for i in range(len(lr_schedule)) if lr_schedule[i][0]==int(epoch)]\n","    if len(epoch_in_sched)>0:\n","        # If it is, return the learning rate corresponding to the epoch\n","        return lr_schedule[epoch_in_sched[0]][1]\n","    else:\n","        # Otherwise, return the existing learning rate\n","        return lr"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h10CF_MEEkux","colab_type":"text"},"source":["Let's now define the callback itself."]},{"cell_type":"code","metadata":{"id":"z68OuB6oEkux","colab_type":"code","colab":{}},"source":["# Define the custom callback\n","\n","class LRScheduler(tf.keras.callbacks.Callback):\n","    \n","    def __init__(self, new_lr):\n","        super(LRScheduler, self).__init__()\n","        # Add the new learning rate function to our callback\n","        self.new_lr = new_lr\n","\n","    def on_epoch_begin(self, epoch, logs=None):\n","        # Make sure that the optimizer we have chosen has a learning rate, and raise an error if not\n","        if not hasattr(self.model.optimizer, 'lr'):\n","              raise ValueError('Error: Optimizer does not have a learning rate.')\n","                \n","        # Get the current learning rate\n","        curr_rate = float(tf.keras.backend.get_value(self.model.optimizer.lr))\n","        \n","        # Call the auxillary function to get the scheduled learning rate for the current epoch\n","        scheduled_rate = self.new_lr(epoch, curr_rate)\n","\n","        # Set the learning rate to the scheduled learning rate\n","        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_rate)\n","        print('Learning rate for epoch {} is {:7.3f}'.format(epoch, scheduled_rate))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oqAwbHjwEkuz","colab_type":"text"},"source":["Let's now train the model again with our new callback. "]},{"cell_type":"code","metadata":{"id":"hyKl5dRYEkuz","colab_type":"code","colab":{}},"source":["# Build the same model as before\n","\n","new_model = tf.keras.Sequential([\n","    Dense(128, activation='relu', input_shape=(train_data.shape[1],)),\n","    Dense(64,activation='relu'),\n","    tf.keras.layers.BatchNormalization(),\n","    Dense(64, activation='relu'),\n","    Dense(64, activation='relu'),\n","    Dense(1)        \n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hnrdQPeqEku1","colab_type":"code","colab":{}},"source":["# Compile the model\n","\n","new_model.compile(loss='mse',\n","                optimizer=\"adam\",\n","                metrics=['mae', 'mse'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wKtNOOITEku2","colab_type":"code","colab":{}},"source":["# Fit the model with our learning rate scheduler callback\n","\n","new_history = new_model.fit(train_data, train_targets, epochs=20,\n","                            batch_size=100, callbacks=[LRScheduler(get_new_epoch_lr)], verbose=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CGKynDkoEku5","colab_type":"text"},"source":["### Further reading and resources\n","* https://www.tensorflow.org/guide/keras/custom_callback\n","* https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback"]}]}